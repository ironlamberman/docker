# #######################
# # Relay Configuration #
# #######################
# This config file only applies for carbon relays, that is
# daemons whose WORKFLOW ends in 'relay'.
#
# RELAY_METHOD can be one of the following:
#   relay-rules
#     Use relay-rules.conf to route metrics to destinations based on pattern rules
#   consistent-hashing
#     Use consistent-hashing for even distribution of metrics between destinations
#   aggregated-consistent-hashing
#     Use consistent-hashing but take into account an aggregation-rules.conf
#     shared by downstream carbon-aggregator daemons. This will ensure that all
#     metrics that map to a given aggregation rule are sent to the same
#     carbon-aggregator instance.
#     Enable this for carbon-relays that send to a group of carbon-aggregators
#
#RELAY_METHOD = consistent-hashing
#RELAY_METHOD = aggregated-consistent-hashing
RELAY_METHOD = relay-rules

# If you use consistent-hashing you may want to add redundancy
# of your data by replicating every datapoint to more than
# one machine. This has no effect with relay-rules.
REPLICATION_FACTOR = 1

# This is a list of carbon daemons we will send any relayed or
# generated metrics to. The default provided would send to a single
# carbon-cache instance on the default port. However if you
# use multiple carbon-cache instances then it would look like this:
#
# DESTINATIONS = 127.0.0.1:2004:cache0, 127.0.0.1:2104:cache1
#
# The general form is IP:PORT:INSTANCE where the :INSTANCE part
# USED TO BE optional but is now required!
#
# Note that if the destinations are all caches then this should
# match the IPs listed in the webapp's CARBONLINK_HOSTS setting
# (though the ports will differ). Order matters!
DESTINATIONS = 127.0.0.1:2004:cache0

# This is the maximum number of datapoints that can be queued up
# for a single destination. Once this limit is hit, we will
# stop accepting new data if USE_FLOW_CONTROL is True, otherwise
# we will drop any subsequently received datapoints.
MAX_QUEUE_SIZE = 10000

# This defines the maximum "message size" between carbon daemons.  If
# your queue is large, setting this to a lower number will cause the
# relay to forward smaller discrete chunks of stats, which may prevent
# overloading on the receiving side after a disconnect.
MAX_DATAPOINTS_PER_MESSAGE = 500

# This is the percentage that the queue must be empty before it will accept
# more messages.  For a larger site, if the queue is very large it makes sense
# to tune this to allow for incoming stats.  So if you have an average
# flow of 100k stats/minute, and a MAX_QUEUE_SIZE of 3,000,000, it makes sense
# to allow stats to start flowing when you've cleared the queue to 95% since
# you should have space to accommodate the next minute's worth of stats
# even before the relay incrementally clears more of the queue
QUEUE_LOW_WATERMARK_PCT = 0.8

# To allow for batch efficiency from the pickle protocol and to benefit from
# other batching advantages, all writes are deferred by putting them into a queue,
# and then the queue is flushed and sent a small fraction of a second later.
TIME_TO_DEFER_SENDING = 0.0001

# Set this to True to pause receiving metrics when the size of a queue to any of the
# DESTINATIONS hits MAX_QUEUE_SIZE. If this happens, sockets over which metrics
# are received will temporarily stop accepting data until the send queues fall
# below 80% of MAX_QUEUE_SIZE. If this is set to False (the default), received
# points will be dropped when the metric cache fills.
USE_FLOW_CONTROL = False

# If you're connecting from the relay to a destination that's over the
# internet or similarly iffy connection, a backlog can develop because
# of internet weather conditions, e.g. acks getting lost or similar issues.
# To deal with that, you can enable USE_RATIO_RESET which will let you
# re-set the connection to an individual destination.  Defaults to being off.
USE_RATIO_RESET=False

# When there is a small number of stats flowing, it's not desirable to
# perform any actions based on percentages - it's just too "twitchy".
MIN_RESET_STAT_FLOW=1000

# When the ratio of stats being sent in a reporting interval is far
# enough from 1.0, we will disconnect the socket and reconnecto to
# clear out queued stats.  The default ratio of 0.9 indicates that 10%
# of stats aren't being delivered within one CARBON_METRIC_INTERVAL
# (default of 60 seconds), which can lead to a queue backup.  Under
# some circumstances re-setting the connection can fix this, so
# set this according to your tolerance, and look in the logs for
# "resetConnectionForQualityReasons" to observe whether this is kicking
# in when your sent queue is building up.
MIN_RESET_RATIO=0.9

# The minimum time between resets.  When a connection is re-set, we
# need to wait before another reset is performed.
# (2*CARBON_METRIC_INTERVAL) + 1 second is the minimum time needed
# before stats for the new connection will be available.  Setting this
# below (2*CARBON_METRIC_INTERVAL) + 1 second will result in a lot of
# reset connections for no good reason.
MIN_RESET_INTERVAL=121
